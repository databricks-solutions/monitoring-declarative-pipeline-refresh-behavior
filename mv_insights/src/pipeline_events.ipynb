{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from concurrent.futures import FIRST_COMPLETED, ThreadPoolExecutor, wait\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from databricks.connect import DatabricksSession\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = \"main\"\n",
    "schema = \"incremental_dlt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = DatabricksSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize client and auth\n",
    "w = WorkspaceClient()\n",
    "pipelines = w.pipelines.list_pipelines()\n",
    "token_value = w.tokens.create(comment=f\"sdk-{time.time_ns()}\").token_value\n",
    "host = w.config.host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare headers\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {token_value}\"\n",
    "}\n",
    "\n",
    "# Worker function to fetch events for a pipeline\n",
    "def fetch_events(pipeline):\n",
    "    \"\"\"Fetch pipeline event logs from the Databricks API for a given pipeline.\n",
    "\n",
    "    Args:\n",
    "        pipeline: An object containing a `pipeline_id` attribute.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary or JSON response containing pipeline events.\n",
    "\n",
    "    \"\"\"\n",
    "    pipeline_id = pipeline.pipeline_id\n",
    "    url = f\"{host}/api/2.0/pipelines/{pipeline_id}/events\"\n",
    "    query_params = {}\n",
    "    pipeline_events = []\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            response = requests.get(url, headers=headers, params=query_params)\n",
    "            response.raise_for_status()\n",
    "            resp = response.json()\n",
    "            events = resp.get(\"events\", [])\n",
    "            if not events:\n",
    "                break  # Skip if no events\n",
    "\n",
    "            # Tag events with pipeline info\n",
    "            for event in events:\n",
    "                event[\"pipeline_id\"] = pipeline_id\n",
    "                event[\"pipeline_name\"] = pipeline.name\n",
    "                pipeline_events.append(event)\n",
    "\n",
    "            # Set up pagination for the next page\n",
    "            if 'next_page_token' in resp:\n",
    "                query_params[\"page_token\"] = resp[\"next_page_token\"]\n",
    "            else:\n",
    "                break  # No more pages to fetch\n",
    "\n",
    "        print(f\"Fetched {len(pipeline_events)} events for {pipeline.name}\")\n",
    "        return pipeline_events\n",
    "    except Exception as e:\n",
    "        print(f\"Failed for {pipeline.name}: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_threads = 5\n",
    "eventful_pipeline_limit = 100\n",
    "eventful_pipeline_count = 0\n",
    "all_events = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=max_threads) as executor:\n",
    "    # Generator to control submission\n",
    "    pipeline_iter = iter(pipelines)\n",
    "    running_futures = []\n",
    "\n",
    "    while eventful_pipeline_count < eventful_pipeline_limit:\n",
    "        # Fill up the thread pool\n",
    "        while len(running_futures) < max_threads:\n",
    "            try:\n",
    "                p = next(pipeline_iter)\n",
    "                future = executor.submit(fetch_events, p)\n",
    "                running_futures.append(future)\n",
    "            except StopIteration:\n",
    "                break  # No more pipelines to submit\n",
    "\n",
    "        # Wait for any future to complete\n",
    "        done, _ = wait(running_futures, return_when=FIRST_COMPLETED)\n",
    "        for future in done:\n",
    "            running_futures.remove(future)\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                all_events.extend(result)\n",
    "                eventful_pipeline_count += 1\n",
    "                if eventful_pipeline_count >= eventful_pipeline_limit:\n",
    "                    break\n",
    "\n",
    "print(\n",
    "    f\"\\nCollected events from {eventful_pipeline_count} pipelines in \"\n",
    "    f\"{round(time.time() - start, 2)} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "all_pipelines_df = pd.DataFrame(all_events)\n",
    "len(all_pipelines_df['pipeline_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.createDataFrame(all_pipelines_df)\n",
    "spark_df.createOrReplaceTempView(\"demo_event_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT id, sequence, origin, timestamp, message, level, event_type, maturity_level, pipeline_id, pipeline_name, error FROM demo_event_log\"\n",
    "\n",
    "result_df = spark.sql(query)\n",
    "result_df.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(f\"{catalog}.{schema}.raw_event_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH refresh_method AS (\n",
    "  SELECT \n",
    "    origin.org_id, \n",
    "    origin.pipeline_id,\n",
    "    timestamp, \n",
    "    origin.pipeline_name, \n",
    "    REGEXP_EXTRACT(message, 'executed as ([A-Z_]+)', 1) AS refresh_type,\n",
    "\n",
    "    -- maintenance type\n",
    "    CASE \n",
    "      WHEN get(details.planning_information.technique_information, 0).is_chosen = true \n",
    "        THEN get(\n",
    "          details.planning_information.technique_information, 0\n",
    "        ).maintenance_type \n",
    "      WHEN get(details.planning_information.technique_information, 1).is_chosen = true \n",
    "        THEN get(\n",
    "          details.planning_information.technique_information, 1\n",
    "        ).maintenance_type \n",
    "      WHEN get(details.planning_information.technique_information, 2).is_chosen = true \n",
    "        THEN get(\n",
    "          details.planning_information.technique_information, 2\n",
    "        ).maintenance_type \n",
    "      WHEN get(details.planning_information.technique_information, 3).is_chosen = true \n",
    "        THEN get(\n",
    "          details.planning_information.technique_information, 3\n",
    "        ).maintenance_type \n",
    "      ELSE NULL \n",
    "    END AS chosen_maintenance_type,\n",
    "\n",
    "    -- cost\n",
    "    CASE \n",
    "      WHEN get(details.planning_information.technique_information, 0).is_chosen = true \n",
    "        THEN get(details.planning_information.technique_information, 0).cost\n",
    "      WHEN get(details.planning_information.technique_information, 1).is_chosen = true \n",
    "        THEN get(details.planning_information.technique_information, 1).cost\n",
    "      WHEN get(details.planning_information.technique_information, 2).is_chosen = true \n",
    "        THEN get(details.planning_information.technique_information, 2).cost\n",
    "      WHEN get(details.planning_information.technique_information, 3).is_chosen = true \n",
    "        THEN get(details.planning_information.technique_information, 3).cost\n",
    "      ELSE NULL \n",
    "    END AS cost,\n",
    "\n",
    "    -- recompute reason\n",
    "    CASE \n",
    "      WHEN (\n",
    "        CASE \n",
    "          WHEN get(details.planning_information.technique_information, 0).is_chosen = true \n",
    "            THEN get(\n",
    "              details.planning_information.technique_information, 0\n",
    "            ).maintenance_type \n",
    "          WHEN get(details.planning_information.technique_information, 1).is_chosen = true \n",
    "            THEN get(\n",
    "              details.planning_information.technique_information, 1\n",
    "            ).maintenance_type \n",
    "          WHEN get(details.planning_information.technique_information, 2).is_chosen = true \n",
    "            THEN get(\n",
    "              details.planning_information.technique_information, 2\n",
    "            ).maintenance_type \n",
    "          WHEN get(details.planning_information.technique_information, 3).is_chosen = true \n",
    "            THEN get(\n",
    "              details.planning_information.technique_information, 3\n",
    "            ).maintenance_type \n",
    "          ELSE NULL \n",
    "        END\n",
    "      ) = 'MAINTENANCE_TYPE_COMPLETE_RECOMPUTE' \n",
    "      THEN COALESCE(\n",
    "        CASE \n",
    "          WHEN get(\n",
    "            get(details.planning_information.technique_information, 1)\n",
    "              .incrementalization_issues, 0\n",
    "          ).issue_type = 'EXPECTATIONS_NOT_SUPPORTED' \n",
    "            THEN 'EXPECTATIONS_NOT_SUPPORTED' \n",
    "          ELSE NULL \n",
    "        END,\n",
    "        CASE \n",
    "          WHEN get(\n",
    "            get(details.planning_information.technique_information, 2)\n",
    "              .incrementalization_issues, 0\n",
    "          ).issue_type = 'EXPECTATIONS_NOT_SUPPORTED' \n",
    "            THEN 'EXPECTATIONS_NOT_SUPPORTED' \n",
    "          ELSE NULL \n",
    "        END,\n",
    "        get(\n",
    "          get(details.planning_information.technique_information, 0)\n",
    "            .incrementalization_issues, 0\n",
    "        ).issue_type,\n",
    "        get(\n",
    "          get(details.planning_information.technique_information, 1)\n",
    "            .incrementalization_issues, 0\n",
    "        ).issue_type,\n",
    "        get(\n",
    "          get(details.planning_information.technique_information, 2)\n",
    "            .incrementalization_issues, 0\n",
    "        ).issue_type,\n",
    "        get(\n",
    "          get(details.planning_information.technique_information, 3)\n",
    "            .incrementalization_issues, 0\n",
    "        ).issue_type,\n",
    "        'UNKNOWN_ISSUE'\n",
    "      )\n",
    "      ELSE 'incremental recompute'\n",
    "    END AS recompute_reason\n",
    "  FROM demo_event_log\n",
    "  WHERE event_type = 'planning_information'\n",
    "),\n",
    "\n",
    "flow_info AS (\n",
    "  SELECT \n",
    "    origin.org_id, \n",
    "    details.flow_definition.output_dataset AS output_dataset,\n",
    "    details.flow_definition.explain_text AS query,\n",
    "    details.flow_definition.flow_type AS table_type\n",
    "  FROM demo_event_log\n",
    "  WHERE event_type = 'flow_definition'\n",
    ")\n",
    "\n",
    "SELECT \n",
    "  r.timestamp,\n",
    "  r.pipeline_id,\n",
    "  r.pipeline_name,\n",
    "  r.refresh_type,\n",
    "  r.chosen_maintenance_type,\n",
    "  r.recompute_reason,\n",
    "  r.cost,\n",
    "  f.output_dataset, \n",
    "  f.query, \n",
    "  f.table_type\n",
    "FROM refresh_method r\n",
    "LEFT JOIN flow_info f \n",
    "  ON r.org_id = f.org_id\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = spark.sql(query)\n",
    "result_df.write.mode(\"overwrite\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .saveAsTable(f\"{catalog}.{schema}.combined_event_log\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "notebook",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
